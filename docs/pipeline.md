---

# ğŸ MLX-Gemma Pipeline: PPO vs GRPO Comparison Study (2026)

**Hedef DonanÄ±m:** Apple Silicon (M1/M2/M3)  
**KÃ¼tÃ¼phane:** `mlx`, `mlx-lm` (Python)  
**Ana Model:** `google/gemma-2b-it` (4-bit Quantized)  
**Veri Seti:** `HuggingFaceH4/ultrafeedback_binarized`  
**Alignment Methods:** **PPO** (Proximal Policy Optimization) vs **GRPO** (Group Relative Policy Optimization)  
**Reward Model:** **Bradley-Terry** (Pairwise Preference, chosen/rejected ile eÄŸitilir)

---

## ğŸ”¬ **Experimental Design: PPO vs GRPO**

Bu pipeline, PPO ve GRPO algoritmalarÄ±nÄ± kontrollÃ¼ bir ortamda karÅŸÄ±laÅŸtÄ±rmak iÃ§in tasarlanmÄ±ÅŸtÄ±r. SonuÃ§lar deneysel verilerle belirlenir.

### KontrollÃ¼ DeÄŸiÅŸkenler (Sabit Tutulanlar)

| DeÄŸiÅŸken | DeÄŸer |
|----------|-------|
| **SFT Base Model** | `sft_merged_model` (Faz 1 Ã§Ä±ktÄ±sÄ±) |
| **Reward Model** | `rm_model_bt` (Faz 2 Ã§Ä±ktÄ±sÄ±, Bradley-Terry) |
| **Training Data** | `HuggingFaceH4/ultrafeedback_binarized` |
| **Evaluation Data** | 300 stratified prompt (100 factual, 100 instruction, 100 creative) |
| **Random Seeds** | `[42, 123, 777]` (her yÃ¶ntem iÃ§in 3 run) |
| **Max Sequence Length** | 512 |
| **Base Learning Rate** | 5e-6 |
| **Batch Size** | 16 |
| **Num Iterations** | 800 |
| **KL Target (Phased)** | 0.05 â†’ 0.02 â†’ 0.01 (aÅŸamalÄ±) |

### DeÄŸiÅŸen DeÄŸiÅŸkenler (YalnÄ±zca Bunlar)

| DeÄŸiÅŸken | PPO (Faz 3A) | GRPO (Faz 3B) |
|----------|-------------|--------------|
| **Algorithm** | PPO + Critic | GRPO (Group-based) |
| **Advantage Estimation** | GAE (Î»=0.95, Î³=0.99) | Group-relative |
| **Critic Model** | Var (Value function) | Yok |
| **clip_range** | 0.2 | 0.2 |
| **value_coef** | 0.5 | â€” |
| **entropy_coef** | 0.01 | â€” |
| **Group Size K** | â€” | 6 |

### Success Metrics

| Metrik | AÃ§Ä±klama | Ã–lÃ§Ã¼m YÃ¶ntemi |
|--------|----------|---------------|
| **Win Rate vs SFT** | Final modelin SFT baseline'Ä± yenme oranÄ± | **GPT-4o-mini** (position-swap debiased) |
| **Tie Rate** | Position-swap tutarsÄ±zlÄ±k oranÄ± (yÃ¼ksek â†’ judge gÃ¼venilmez) | GPT-4o-mini (A-B â‰  B-A â†’ tie) |
| **KL Divergence** | Policy drift (dÃ¼ÅŸÃ¼k = daha stabil) | Ortalama KL(actor â€– reference) |
| **Perplexity** | Ãœretim kalitesi | Log-likelihood |
| **Training Time** | Toplam eÄŸitim sÃ¼resi | Saat (M2 Pro 16GB) |
| **Peak VRAM** | Maksimum bellek kullanÄ±mÄ± | `mx.metal.get_peak_memory()` |
| **Convergence Speed** | Win rate %60'a ulaÅŸma iterasyonu | Training curve |
| **Variance Across Runs** | 3 seed arasÄ±ndaki std sapmasÄ± | Std(win_rate) |
| **Win Rate 95% CI** | Win rate hata payÄ± (Wilson Score Interval) | `p Â± 1.96 * sqrt(p(1-p)/N)` |
| **Cohen's d** | Effect size (birincil karÅŸÄ±laÅŸtÄ±rma metriÄŸi) | Standardized mean difference |
| **Avg Response Length** | Verbosity bias kontrolÃ¼ (uzun = iyi mi?) | Token count diff (PPO vs SFT) |
| **Mean Reward Score** | Reward inflation kontrolÃ¼ | Average RM score over time |

> [!WARNING]
> **Gizli Tehlikeler: Verbosity Bias & Reward Inflation**
> *   **Verbosity Bias (Gevezelik):** Model sadece "daha uzun" yazdÄ±ÄŸÄ± iÃ§in Ã¶dÃ¼l alÄ±yor olabilir. PPO cevabÄ± SFT'den 2 kat uzunsa ama Win Rate artmadÄ±ysa, baÅŸarÄ± sahtedir.
> *   **Reward Inflation (Puan ÅiÅŸmesi):** Reward Model zamanla hep daha yÃ¼ksek puanlar verebilir (3.0 -> 9.0). EÄŸer Win Rate sabit kalÄ±yorsa, bu puan artÄ±ÅŸÄ± aÅŸÄ±rÄ± optimizasyondur (Overoptimization).

> [!NOTE]
> **Neden Confidence Interval (%95 CI)?**
> Sadece "Win Rate %65" demek yetmez. Hata payÄ±nÄ± bilmek gerekir.
> *   EÄŸer PPO **%65 (Â±%4)** ve GRPO **%60 (Â±%8)** ise, aralÄ±klarÄ± **Ã¶rtÃ¼ÅŸÃ¼r (%61-%69 vs %52-%68)**.
> *   Bu durumda "PPO kesinlikle daha iyidir" diyemeyiz. CI bize **istatistiksel gÃ¼venilirliÄŸi** gÃ¶sterir.

> [!NOTE]
> **Neden Cohen's d? (Effect Size vs Significance)**
> Sadece "PPO kazandÄ±" (p-value) demek yetmez, Ã§Ã¼nkÃ¼ 3 run ile istatistiksel anlamlÄ±lÄ±k (significance) yakalamak zordur. Cohen's d bize **"Fark ne kadar bÃ¼yÃ¼k?"** (Effect Size) sorusunun cevabÄ±nÄ± verir.
> *   **0.2 (Small):** Fark var ama ÅŸans eseri olabilir.
> *   **0.5 (Medium):** Belirgin bir Ã¼stÃ¼nlÃ¼k var.
> *   **0.8 (Large):** Ezici bir Ã¼stÃ¼nlÃ¼k var (Variance'dan etkilenmiyor).

> [!TIP]
> **Neden GPT-4o-mini & Position Swap?**
> *   **Hakem (Judge):** Matematiksel bir "doÄŸru cevap" olmadÄ±ÄŸÄ± iÃ§in, cevabÄ±n kalitesini bir insan gibi deÄŸerlendirecek baÅŸka bir modele (GPT-4o-mini) ihtiyacÄ±mÄ±z var. Maliyeti dÃ¼ÅŸÃ¼k, hÄ±zÄ± yÃ¼ksek (~$1).
> *   **Position Swap:** JÃ¼ri modelleri genelde "ilk cevabÄ±" seÃ§me eÄŸilimindedir (Bias). Bunu kÄ±rmak iÃ§in her maÃ§Ä± iki kere yaptÄ±rÄ±rÄ±z: (A vs B) ve (B vs A). EÄŸer ikisinde de aynÄ± model kazanÄ±rsa sonuÃ§ geÃ§erlidir (Tie Rate dÃ¼ÅŸÃ¼k olmalÄ±).

---

## ğŸ **FAZ 1: SFT (Supervised Fine-Tuning)**

Modelin talimatlarÄ± anlamasÄ± ve UltraFeedback kalitesine alÄ±ÅŸmasÄ± iÃ§in yapÄ±lan Ä±sÄ±nma turu. Bu faz her iki yÃ¶ntem iÃ§in **ortaktÄ±r**.

* **Girdi:** UltraFeedback veri setindeki `chosen` cevaplar.
* **Teknoloji:** **QLoRA** (Quantized Low-Rank Adaptation).

### ğŸ§  Model Durumu (Phase 1)

| Model | Tip | Durum | MLX YapÄ±landÄ±rmasÄ± |
|-------|-----|-------|-------------------|
| **Gemma 2B** | Actor | ğŸ”´ **EÄÄ°TÄ°LÄ°YOR (LoRA)** | `--quantize 4bit`, `--rank 16`, `--lora-layers 16` |

### ğŸ“Š Hiperparametreler

| Parametre | DeÄŸer |
|-----------|-------|
| `learning_rate` | 2e-4 |
| `batch_size` | 4 |
| `gradient_accumulation_steps` | 4 (efektif batch = 16) |
| `epochs` | 3 |
| `warmup_steps` | 100 |
| `max_seq_length` | 512 |

### ğŸ” Modele Ne Veriliyor? (GenRM FormatÄ±)

SFT aÅŸamasÄ±nda model bir **Generative Reward Model (GenRM)** olarak eÄŸitilir. Yani model, bir cevabÄ±n kalitesini deÄŸerlendirmeyi Ã¶ÄŸrenir.

Tokenizer, `train.jsonl`'deki her satÄ±rÄ± Gemma chat template ile ÅŸu sequence'a Ã§evirir:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ <bos><start_of_turn>user                                           â”‚
â”‚ User: Write a eulogy for a public figure who inspired you.         â”‚
â”‚                                                                     â”‚
â”‚ Assistant: Ladies and gentlemen, we gather here today to celebrate  â”‚
â”‚ the life and legacy of...                                          â”‚
â”‚                                                                     â”‚
â”‚ Analyze the quality of this response.                              â”‚
â”‚ <end_of_turn>                                                       â”‚
â”‚ <start_of_turn>model                                               â”‚
â”‚ Score: 8.5/10. The response is helpful, harmless, and honest.      â”‚  â—„â”€â”€ MODEL BUNU ÃœRETMEYÄ° Ã–ÄRENÄ°R
â”‚ <end_of_turn><eos>                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²                                           â–²
         â”‚                                           â”‚
    CONTEXT (input)                          TARGET (loss hesaplanÄ±r)
```

> [!IMPORTANT]
> Model **soru cevaplamayÄ± deÄŸil**, verilen bir cevabÄ± **puanlamayÄ±** Ã¶ÄŸreniyor. Bu yÃ¼zden "user" mesajÄ±nÄ±n iÃ§inde hem prompt hem de chosen response birlikte yer alÄ±r.

### ğŸ—ºï¸ Faz BazlÄ± Model Girdi/Ã‡Ä±ktÄ± Ã–zeti

| Faz | Modele Giren | Modelden Ã‡Ä±kan | AmaÃ§ |
|-----|-------------|----------------|------|
| **1. SFT** | prompt + chosen response + "Analyze..." | `"Score: X/10..."` | GenRM olarak skor Ã¼retmeyi Ã¶ÄŸren |
| **2. RM (BT)** | prompt + chosen / rejected (ayrÄ± ayrÄ±) | scalar reward `r(x,y)` | Chosen > rejected sÄ±ralamasÄ±nÄ± Ã¶ÄŸren |
| **3A. PPO** | prompt â†’ Actor response Ã¼retir | RM skorlar â†’ GAE â†’ policy update | YÃ¼ksek reward alan cevaplar Ã¼ret |
| **3B. GRPO** | prompt â†’ Actor K=6 response Ã¼retir | RM skorlar â†’ group-relative advantage | Grup iÃ§inde en iyiyi Ã¶ÄŸren |

> **Ã‡Ä±ktÄ±:** `sft_adapter.npz` â†’ Base Gemma'ya merge edilerek `sft_merged_model` oluÅŸur.

**âš ï¸ KRÄ°TÄ°K:** Bu model her iki alignment yÃ¶ntemi iÃ§in de **reference point**'tir. AynÄ± `sft_merged_model` hem PPO hem GRPO baÅŸlangÄ±cÄ±nda kullanÄ±lÄ±r.

---

## âš–ï¸ **FAZ 2: Bradley-Terry Reward Model EÄŸitimi**

> [!NOTE]
> **EÄŸitimsel Not: Supervised vs Reinforcement Learning**
> Bradley-Terry eÄŸitimi teknik olarak **Supervised Learning**'dir, RL deÄŸildir.
> *   **SFT (Faz 1):** Modele "Cevap ÅŸÃ¶yle olmalÄ±" diyerek **taklit etmeyi** Ã¶ÄŸretiriz.
> *   **RM (Faz 2):** Modele "Hangisi daha iyi?" diyerek **eleÅŸtirmeyi** Ã¶ÄŸretiriz. Bu model maÃ§Ä±n hakemidir.
> *   **RL (Faz 3):** Hakemin oyuna dahil olduÄŸu ve modelin skoru artÄ±rmak iÃ§in Ã§abaladÄ±ÄŸÄ± yer burasÄ±dÄ±r.

> [!IMPORTANT]
> **Pairwise Preference Modeling.** UltraFeedback'teki `chosen` / `rejected` Ã§iftleri kullanÄ±larak Bradley-Terry modeli eÄŸitilir. EÄŸitilen RM, training sÄ±rasÄ±nda her Ã¼retilen cevaba scalar skor verir.

### ğŸ“¦ Veri FormatÄ± (Pairwise)

```json
{
  "prompt": "Python'da liste nasÄ±l sÄ±ralanÄ±r?",
  "chosen": "list.sort() veya sorted(list) kullanabilirsiniz...",
  "rejected": "Python'da liste yok, sadece array var..."
}
```

### ğŸ“¦ GenRM Training Data Format (`train.jsonl`)

`download_data.py`, ham UltraFeedback verisini aÅŸaÄŸÄ±daki GenRM formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r:

```
Raw UltraFeedback                          train.jsonl
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ prompt               â”‚â”€â”€â”                â”‚ messages[0] (role: "user")       â”‚
â”‚ chosen[-1].content   â”‚â”€â”€â”¤â”€â”€ concat â”€â”€â–º   â”‚   "User: {prompt}\n\n            â”‚
â”‚                      â”‚  â”‚                â”‚    Assistant: {chosen}\n\n       â”‚
â”‚                      â”‚  â”‚                â”‚    Analyze the quality..."       â”‚
â”‚ score_chosen         â”‚â”€â”€â”˜â”€â”€ format â”€â”€â–º   â”‚ messages[1] (role: "assistant")  â”‚
â”‚                      â”‚                   â”‚   "Score: {score}/10. ..."       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| `train.jsonl` AlanÄ± | Kaynak | Ä°Ã§erik |
|----------------------|--------|--------|
| `messages[0]` (user) | `prompt` + `chosen` response | Orijinal prompt + chosen cevap + "Analyze the quality..." talimatÄ± |
| `messages[1]` (assistant) | `score_chosen` | `"Score: {score:.1f}/10. The response is helpful, harmless, and honest."` |
| Score | YalnÄ±zca `score_chosen` | Regex ile parse: `Score:\s*([0-9]+(?:\.[0-9]+)?)/10` |

> [!IMPORTANT]
> YalnÄ±zca `score_chosen` kullanÄ±lÄ±r â€” rejected response'un skoru training verisinde yer almaz. "User" mesajÄ±, prompt **ve** chosen response'u birlikte iÃ§erir.

**EDA Notebook'taki Kolon KarÅŸÄ±lÄ±klarÄ±:**

| Notebook Kolonu | GerÃ§ek Ä°Ã§erik | AÃ§Ä±klama |
|-----------------|---------------|----------|
| `user_tokens` | prompt + chosen response + instruction | Fine-tuning input uzunluÄŸu (uzun) |
| `assistant_tokens` | `"Score: X/10..."` | Sadece skor cÃ¼mlesi (~15 token, Ã§ok kÄ±sa) |
| `chat_tokens` | user + assistant + chat template overhead | Toplam sequence uzunluÄŸu (fine-tuning'deki gerÃ§ek uzunluk) |
| `score` | `score_chosen` | 0â€“10 arasÄ± float |

### ğŸ§  Bradley-Terry Model YapÄ±sÄ±

Model, SFT checkpoint'inden baÅŸlayan bir Gemma backbone'u + scalar reward head'den oluÅŸur:

```python
class BradleyTerryRM:
    def __init__(self, base_model_path):
        self.gemma = load_model_4bit(base_model_path)     # Shared backbone
        self.reward_head = nn.Linear(2048, 1)              # Scalar r(x, y)

    def __call__(self, tokens):
        hidden = self.gemma(tokens, output_hidden_states=True)
        return self.reward_head(hidden[-1][:, -1, :]).squeeze(-1)

    def get_reward(self, prompt, response) -> float:
        """Tek bir (prompt, response) Ã§ifti iÃ§in scalar reward."""
        ...
```

### ğŸ§© Bradley-Terry Loss

```
P(chosen > rejected) = Ïƒ(r_chosen âˆ’ r_rejected)
Loss = âˆ’log Ïƒ(r_chosen âˆ’ r_rejected)
```

```python
def bradley_terry_loss(rm_model, prompt, chosen, rejected):
    r_chosen  = rm_model.get_reward(prompt, chosen)
    r_rejected = rm_model.get_reward(prompt, rejected)
    return -mx.log(mx.sigmoid(r_chosen - r_rejected) + 1e-8)
```

### ğŸ”„ RM Training

`train_bradley_terry_rm(rm_model, dataset, epochs, lr, batch_size)` fonksiyonu ile UltraFeedback chosen/rejected Ã§iftleri Ã¼zerinde eÄŸitilir. Her epoch sonunda pairwise accuracy raporlanÄ±r.

### ğŸ“Š RM Hiperparametreleri

| Parametre | DeÄŸer |
|-----------|-------|
| `learning_rate` | 1e-4 |
| `batch_size` | 8 |
| `epochs` | 2 |
| `lora_rank` | 8 |
| `lora_alpha` | 16 |
| `max_seq_length` | 1024 |

> **Ã‡Ä±ktÄ±:** `rm_model_bt` â†’ Bradley-Terry RM. EÄŸitim sonrasÄ±nda pairwise accuracy â‰¥ 0.70 beklenir.

> [!CAUTION]
> **Sanity Check: Calibration Error (ECE)**
> Accuracy tek baÅŸÄ±na yetmez! Model "%90 eminim" dediÄŸinde gerÃ§ekten %90 haklÄ± mÄ± Ã§Ä±kÄ±yor?
> *   **Expected Calibration Error (ECE):** < 0.1 olmalÄ±.
> *   EÄŸer ECE yÃ¼ksekse (model aÅŸÄ±rÄ± Ã¶zgÃ¼venli), PPO sÄ±rasÄ±nda yanlÄ±ÅŸ cevaplara Ã§ok yÃ¼ksek Ã¶dÃ¼l verebilir (Reward Hacking).
> *   Ã‡Ã¶zÃ¼m: `Temperature Scaling` ile kalibre edilir.

---

## âš–ï¸ **FAZ 3A: PPO Implementation**

Proximal Policy Optimization, bir **Critic (value) network** kullanarak advantage tahminini Ã¶ÄŸrenen bir actor-critic yÃ¶ntemidir. GAE ile varyansÄ± kontrol altÄ±nda tutar.

> [!NOTE]
> **PPO Sahnesindeki Oyuncular (Cast of Characters)**
> EÄŸitim sÄ±rasÄ±nda VRAM'de aslÄ±nda 4 farklÄ± model/kopya bulunur:
> 1.  **Actor (Oyuncu):** EÄŸittiÄŸimiz asÄ±l model. SÃ¼rekli konuÅŸur ve kendini geliÅŸtirir.
> 2.  **Critic (EleÅŸtirmen):** Actor'Ã¼n her cÃ¼mlesine anlÄ±k puan tahmini yapar ("Bence bu cÃ¼mle 7 puanla bitecek").
> 3.  **Reference (Referans):** SFT modelinin donmuÅŸ (frozen) kopyasÄ±. "Eskiden nasÄ±l konuÅŸuyorduk?" diye bakmak iÃ§in durur (KL Anchor).
> 4.  **Reward Model (Hakem):** Cevap bittiÄŸinde son notu (8.5/10) verir.

> [!TIP]
> **GÃ¼venli Ã–ÄŸrenme MekanizmalarÄ±**
> PPO'yu "gÃ¼venli" ve "kararlÄ±" yapan iki temel fren mekanizmasÄ± vardÄ±r:
> *   **Clipping ("Ã‡ok HÄ±zlÄ± DeÄŸiÅŸme!"):** Actor bazen aÅŸÄ±rÄ± heyecanlanÄ±p tÃ¼m bildiklerini deÄŸiÅŸtirmek isteyebilir. PPO, her gÃ¼ncellemede deÄŸiÅŸimi **%20 (clip_range=0.2)** ile sÄ±nÄ±rlar.
> *   **KL Penalty ("Eski Halini Unutma!"):** Model sadece yÃ¼ksek puan almaya odaklanÄ±rsa saÃ§malayabilir (Reward Hacking). Reference model devreye girer: "Eski halinden Ã§ok uzaklaÅŸtÄ±n!" diyerek ceza keser.
> *   **Adaptive KL Controller:** Bu ceza katsayÄ±sÄ± (`beta`) sabit deÄŸildir. Model Ã§ok sapÄ±tÄ±rsa ceza artar, Ã§ok korkak davranÄ±rsa ceza azalÄ±r (Otopilot).

### ğŸ¯ PPO AkÄ±ÅŸÄ±

```
Prompt â†’ Actor.generate(response) â†’ BT RM â†’ scalar reward
                                  â†’ EOS-Only Reward Shaping (aÅŸaÄŸÄ±ya bkz.)
                                  â†’ Critic â†’ V(s_t) (her token iÃ§in)
                                  â†’ GAE: A_t = Î£(Î³Î»)^k Â· Î´_{t+k}
                                  â†’ PPO Clip: L = E[min(ratioÂ·A, clip(ratio,1Â±Îµ)Â·A)]
```

### ğŸ› ï¸ Critic Model

Actor ile aynÄ± backbone'u paylaÅŸan, ayrÄ± bir value head'e sahip aÄŸ. Her state'ten beklenen kÃ¼mÃ¼latif Ã¶dÃ¼lÃ¼ tahmin eder:

```python
class GemmaCriticModel:
    def __init__(self, base_model_path):
        self.gemma = load_model_4bit(base_model_path)
        self.value_head = nn.Linear(2048, 1)  # V(s) tahmini

    def __call__(self, tokens) -> scalar:
        ...
```

### ğŸ”¢ Reward Shaping: EOS-Only Assignment

> [!IMPORTANT]
> **BT RM tÃ¼m response iÃ§in tek bir scalar skor verir.** Ancak GAE, her token adÄ±mÄ± (t) iÃ§in ayrÄ± bir `r_t` bekler. Bu uyumsuzluÄŸu Ã§Ã¶zmek iÃ§in standart yÃ¶ntem kullanÄ±lÄ±r: **reward sadece son token'a (EOS) atanÄ±r**, ara token'larda `r_t = 0`.

```python
def shape_rewards_for_gae(rm_scalar_reward, response_length):
    """
    BT RM'nin tek scalar reward'Ä±nÄ± GAE-uyumlu per-token reward dizisine Ã§evirir.
    r_t = 0  (t < T-1)    â€” ara token'lar
    r_T = rm_reward        â€” EOS token
    """
    rewards = [0.0] * (response_length - 1) + [rm_scalar_reward]
    return rewards
```

Bu sayede GAE, EOS'a kadar olan tÃ¼m token'lar iÃ§in advantage'Ä± geriye doÄŸru yayar (temporal credit assignment).

### ğŸ”¢ GAE (Generalized Advantage Estimation)

```python
def compute_gae(rewards, values, next_values, dones, gamma=0.99, lam=0.95):
    """
    Î´_t = r_t + Î³Â·V(s_{t+1})Â·(1-done) - V(s_t)
    A_t = Î´_t + (Î³Î»)Â·(1-done)Â·A_{t+1}
    SonuÃ§ normalize edilir (mean=0, std=1).
    """
    ...
```

### ğŸ”„ PPO Training Step

Her iterasyonda:
1. **Rollout:** Her prompt iÃ§in 1 response Ã¼retilir, BT RM'den scalar reward alÄ±nÄ±r.
2. **Reward Shaping:** Scalar reward â†’ EOS-only per-token reward dizisine Ã§evrilir.
3. **Critic:** Her token pozisyonunda V(s_t) tahmin edilir.
4. **GAE:** Per-token rewards + values â†’ advantages hesaplanÄ±r.
5. **Policy Update (mini-batch):**

```python
# PPO Clipped Surrogate Objective
ratio = exp(actor_logprobs - old_logprobs)
L_CLIP = -min(ratio * A, clip(ratio, 1-Îµ, 1+Îµ) * A).mean()

# Value Loss
L_value = ((V_current - returns)Â²).mean()

# Entropy Bonus
L_entropy = -(logp * exp(logp)).mean()

# Total (KL penalty loss'ta YOK â€” adaptive controller zaten Î²'yi dÄ±ÅŸarÄ±dan ayarlÄ±yor)
Loss = L_CLIP + 0.5 * L_value - 0.01 * L_entropy
```

> [!WARNING]
> **KL kontrolÃ¼ yalnÄ±zca adaptive controller ile yapÄ±lÄ±r** (aÅŸaÄŸÄ±ya bkz.). Loss'a ayrÄ±ca KL penalty terimi eklenmez â€” ikisi aynÄ± anda Ã§alÄ±ÅŸÄ±rsa policy neredeyse hiÃ§ hareket edemez (double suppression).

6. **Adaptive KL Controller:** Phased schedule (0.05 â†’ 0.02 â†’ 0.01) etrafÄ±nda `kl_coef` dÄ±ÅŸarÄ±dan ayarlanÄ±r. KL yÃ¼ksekse learning rate dÃ¼ÅŸÃ¼rÃ¼lÃ¼r veya early stop uygulanÄ±r.
7. **Best Checkpoint (KL-Gate â†’ PPL):**

> [!IMPORTANT]
> PPL ve KL tamamen farklÄ± Ã¶lÃ§eklerde (PPL: 2â€“5, KL: 0.01â€“0.05). Ä°kisini toplamak anlamsÄ±z. DoÄŸru mantÄ±k: **Ã¶nce KL threshold'u geÃ§ip geÃ§mediÄŸine bak**, geÃ§iyorsa o checkpoint'i atla, geÃ§miyorsa PPL'e gÃ¶re kaydet.

```python
# Best checkpoint seÃ§imi: KL gate THEN PPL comparison
if eval_kl < target_kl:          # 1. KL threshold'u geÃ§iyor mu?
    if eval_ppl < best_ppl:      # 2. Evet â†’ PPL daha iyi mi?
        best_ppl = eval_ppl
        save_best_checkpoint()
```

### ğŸ§  Model DurumlarÄ± (Phase 3A â€” PPO)

| Model | RolÃ¼ | Durumu | Tahmini VRAM |
|-------|------|--------|-------------|
| **Actor** | Policy | ğŸ”´ EÄÄ°TÄ°LÄ°YOR (4-bit + LoRA rank=16) | ~1.5 GB |
| **Critic** | Value Function | ğŸ”´ EÄÄ°TÄ°LÄ°YOR (4-bit + Value Head) | ~1.5 GB |
| **Reference** | KL Anchor | ğŸ§Š FROZEN (SFT checkpoint) | ~1.5 GB |
| **BT RM** | Reward | ğŸ§Š FROZEN (Faz 2 Ã§Ä±ktÄ±sÄ±) | ~0.5 GB |

**Tahmini Toplam VRAM:** ~4â€“5 GB

### ğŸ“Š PPO Hiperparametreleri

| Parametre | DeÄŸer | AÃ§Ä±klama |
|-----------|-------|----------|
| `clip_range` | 0.2 | Îµ â€” policy ratio clipping |
| `value_coef` | 0.5 | Value loss weight |
| `entropy_coef` | 0.01 | Entropy bonus |
| `ppo_epochs` | 1 | Single-use rollouts (GRPO ile eÅŸit) |
| `gae_gamma` | 0.99 | Discount factor |
| `gae_lambda` | 0.95 | GAE Î» |
| `target_kl` | phased | 0.05 â†’ 0.02 â†’ 0.01 (adaptive controller) |
| `max_grad_norm` | 0.5 | Gradient clipping |
| `reward_shaping` | EOS-only | BT RM scalar â†’ son token'a atanÄ±r |

---

## âš–ï¸ **FAZ 3B: GRPO Implementation**

Group Relative Policy Optimization, her prompt iÃ§in bir grup cevap Ã¼retip bu grup iÃ§indeki **gÃ¶reli karÅŸÄ±laÅŸtÄ±rma** ile advantage hesaplar. Critic aÄŸÄ± gerektirmez.

### ğŸ¯ GRPO AkÄ±ÅŸÄ±

```
Prompt â†’ Actor.generate(K=6 responses) â†’ BT RM â†’ [râ‚, râ‚‚, ..., râ‚†]
                                        â†’ Score Normalization (EMA)
                                        â†’ Group-relative advantage: Aáµ¢ = ráµ¢ - mean(r)
                                        â†’ Clipped Surrogate: L = -min(ratioÂ·A, clip(ratio,1Â±Îµ)Â·A)
```

**Ã–rnek:**
```
Responses:   [4.2, 3.8, 2.1, 1.3, 4.5, 2.8]  (RM scores)
Group Mean:  3.12
Advantages:  [+1.08, +0.68, -1.02, -1.82, +1.38, -0.32]
```

> [!TIP]
> **EÄŸitimsel Not: GRPO = "BÃ¼tÃ§e Dostu PPO"**
> GRPO'nun PPO'dan en bÃ¼yÃ¼k farkÄ± **Critic Modelini Ã§Ã¶pe atmasÄ±dÄ±r.**
> *   **PPO:** "Critic" (ayrÄ± bir model) sÃ¼rekli "KaÃ§ puan alacaÄŸÄ±z?" diye tahmin yapar. Bu VRAM yer.
> *   **GRPO:** Tahmin yapmak yerine aynÄ± soruyu **6 kere** cevaplar. Sonra bu cevaplarÄ±n ortalamasÄ±nÄ± alÄ±r.
> *   **MantÄ±k:** "Ortalamadan (arkadaÅŸlarÄ±mdan) iyi miyim?" sorusuna bakar. EÄŸer grup ortalamasÄ± 5 iken sen 8 aldÄ±ysan, Ã¶dÃ¼llendirilirsin.
> *   **SonuÃ§:** Tek bir modelle (Actor) iÅŸ biter, ekstra Critic modeline gerek kalmaz (VRAM tasarrufu).

### ğŸ”¢ Score Normalization (Drift Ã–nleme)

> [!CAUTION]
> RM skorlarÄ± GRPO sÄ±rasÄ±nda **drift edebilir**. EMA-based running normalization ile Ã¶nlÃ¼yoruz.

```python
class ScoreNormalizer:
    """EMA ile running mean/std gÃ¼ncelleyerek score distribution'Ä± korur."""
    def __init__(self, alpha=0.99):
        self.running_mean, self.running_std = 3.0, 1.0
        self.alpha = alpha

    def normalize(self, scores) -> mx.array:
        # EMA update â†’ normalize â†’ rescale [1.0, 5.0] aralÄ±ÄŸÄ±na clip
        ...
```

### ğŸ”„ GRPO Training Step

Her iterasyonda:
1. **Group Sampling:** Her prompt iÃ§in K=6 response Ã¼retilir, old policy logprobs kaydedilir.
2. **Reward:** BT RM'den skorlar alÄ±nÄ±r, `ScoreNormalizer` ile normalize edilir.
3. **Group-Relative Advantage:** `Aáµ¢ = normalized_ráµ¢ - mean(normalized_r)`
4. **Clipped Surrogate Update (mini-batch):**

```python
# GRPO Clipped Surrogate (PPO-style, Critic yok)
ratio = exp(actor_logprobs - old_logprobs)
L_CLIP = -min(ratio * A, clip(ratio, 1-Îµ, 1+Îµ) * A).mean()

# KL penalty loss'ta YOK â€” adaptive controller dÄ±ÅŸarÄ±dan yÃ¶netir
Loss = L_CLIP
```

> [!WARNING]
> PPO ile aynÄ± ÅŸekilde, KL kontrolÃ¼ **yalnÄ±zca adaptive controller** ile yapÄ±lÄ±r. Loss'a ayrÄ±ca KL terimi eklenmez (double suppression riski).

5. **Adaptive KL Controller:** PPO ile aynÄ± phased schedule paylaÅŸÄ±lÄ±r. KL > target ise learning rate dÃ¼ÅŸÃ¼rÃ¼lÃ¼r veya early stop.
6. **Best Checkpoint (KL-Gate â†’ PPL):** PPO ile aynÄ± mantÄ±k â€” Ã¶nce `KL < target_kl` kontrolÃ¼, sonra PPL karÅŸÄ±laÅŸtÄ±rmasÄ±.

### ğŸ§  Model DurumlarÄ± (Phase 3B â€” GRPO)

| Model | RolÃ¼ | Durumu | Tahmini VRAM |
|-------|------|--------|-------------|
| **Actor** | Policy | ğŸ”´ EÄÄ°TÄ°LÄ°YOR (4-bit + LoRA rank=16) | ~1.5 GB |
| **Reference** | KL Anchor | ğŸ§Š FROZEN (SFT checkpoint) | ~1.5 GB |
| **BT RM** | Reward | ğŸ§Š FROZEN (Faz 2 Ã§Ä±ktÄ±sÄ±) | ~0.5 GB |

**Tahmini Toplam VRAM:** ~2.5 GB

### ğŸ“Š GRPO Hiperparametreleri

| Parametre | DeÄŸer | AÃ§Ä±klama |
|-----------|-------|----------|
| `K` | 6 | Group size |
| `clip_range` | 0.2 | Policy ratio clipping |
| `target_kl` | phased | 0.05 â†’ 0.02 â†’ 0.01 (adaptive controller) |
| `normalizer_alpha` | 0.99 | EMA coefficient |
| `max_grad_norm` | 0.5 | Gradient clipping |

---

## ğŸ“Š **Comparison Protocol**

Her iki yÃ¶ntem **eÅŸdeÄŸer koÅŸullar** altÄ±nda deÄŸerlendirilir.

> [!IMPORTANT]
> **Train/Test Split:** UltraFeedback, training baÅŸlamadan Ã¶nce sabit seed (`seed=42`) ile split edilir. 300 test promptu training'de hiÃ§ kullanÄ±lmaz.

### Evaluation Schedule

- **Training sÄ±rasÄ± (her 50 iter):** Sadece **Perplexity** ve **KL Divergence** raporlanÄ±r.
- **Final eval (eÄŸitim sonunda):** **GPT-4o-mini** ile **win rate**, **loss rate** ve **tie rate** birlikte raporlanÄ±r.
- 3 seed Ã— 2 yÃ¶ntem = **6 experiment run**.

### Final Evaluation (GPT-4o-mini â€” Position-Swap Debiased)

> [!IMPORTANT]
> Position bias Ã¶nlemek iÃ§in her karÅŸÄ±laÅŸtÄ±rma iki farklÄ± sÄ±rayla yapÄ±lÄ±r (A-B ve B-A). TutarsÄ±zlÄ±k varsa **TIE** sayÄ±lÄ±r.

```python
def final_evaluate_gpt4o(actor, reference, test_prompts):
    """
    Her prompt iÃ§in:
      1. verdict_1 = gpt4o_judge(actor_resp, sft_resp)   # Actor=A
      2. verdict_2 = gpt4o_judge(sft_resp, actor_resp)   # Actor=B (swap)
      3. Ä°kisi tutarlÄ± â†’ win/loss; tutarsÄ±z â†’ tie
    Stratified kategorilere gÃ¶re ayrÄ± raporlama (factual, instruction, creative).
    Raporda win/loss/tie Ã¼Ã§lÃ¼sÃ¼ birlikte verilir â€” sadece win rate yanÄ±ltÄ±cÄ± olabilir.
    """
    ...
```

> [!NOTE]
> **Tie rate yÃ¼ksekse** (>%30) GPT-4o-mini judge'Ä±n ayrÄ±m gÃ¼cÃ¼ dÃ¼ÅŸÃ¼k demektir â€” bu durumda win rate yorumlarÄ± temkinli yapÄ±lmalÄ±dÄ±r.

### Statistical Significance

3 seed ile Welch t-test'in gÃ¼cÃ¼ dÃ¼ÅŸÃ¼k olduÄŸundan **Cohen's d** birincil karÅŸÄ±laÅŸtÄ±rma kriteri olarak kullanÄ±lÄ±r. Ancak **3 seed yalnÄ±zca large effect size'Ä± gÃ¼venilir tespit eder**; medium ve small farklar iÃ§in sonuÃ§lar kesin deÄŸildir.

| Cohen's d | Yorum | 3 Seed ile GÃ¼venilirlik |
|-----------|-------|------------------------|
| \|d\| < 0.2 | Negligible | âœ… Tespit edilebilir |
| \|d\| 0.2â€“0.5 | Small | âš ï¸ Yetersiz gÃ¼Ã§, kesin deÄŸil |
| \|d\| 0.5â€“0.8 | Medium | âš ï¸ SÄ±nÄ±rda, temkinli yorumla |
| \|d\| > 0.8 | Large | âœ… Yeterli gÃ¼Ã§ |

```python
def cohens_d(group1, group2):
    """Pooled std ile standardized mean difference."""
    ...
```

> [!WARNING]
> 3 seed ile istatistiksel gÃ¼Ã§ dÃ¼ÅŸÃ¼ktÃ¼r â€” yalnÄ±zca **large** effect size gÃ¼venilir tespit edilir. SonuÃ§lar **medium veya small** Ã§Ä±karsa, seed sayÄ±sÄ± artÄ±rÄ±larak (5â€“10 seed) deney tekrarlanabilir.

---

## ğŸ¯ **Expected Trade-offs Table**

AÅŸaÄŸÄ±daki tablo teorik beklentilere dayanmaktadÄ±r. GerÃ§ek sonuÃ§lar deneysel olarak belirlenecektir.

| Metrik | PPO | GRPO | Beklenen Avantaj |
|--------|-----|------|-----------------|
| **Variance (across runs)** | DÃ¼ÅŸÃ¼k (Critic stabilize eder) | Orta-YÃ¼ksek (stochastic sampling) | PPO |
| **Training Stability** | YÃ¼ksek (GAE + value clipping) | Orta (group size'a baÄŸlÄ±) | PPO |
| **VRAM KullanÄ±mÄ±** | ~4â€“5 GB (Critic ekler) | ~2.5 GB (Critic yok) | GRPO |
| **Compute per Iteration** | YÃ¼ksek (Critic forward + backward) | Orta (K parallel samples) | GRPO |
| **Convergence Speed** | Belirsiz | Belirsiz | TBD |
| **Final Win Rate** | Belirsiz | Belirsiz | TBD |
| **Hyperparameter Sensitivity** | YÃ¼ksek (clip_range, value_coef, Î») | Orta (K, kl_coef) | GRPO |
| **Implementation Complexity** | YÃ¼ksek (Critic + GAE) | Orta (Group sampling) | GRPO |
| **Sample Efficiency** | EÅŸit (`ppo_epochs=1`, reuse disabled â€” fair comparison) | EÅŸit (rollouts tek kullanÄ±m) | EÅŸit |

---

## ğŸ’¡ **M2 Pro & MLX OptimizasyonlarÄ±**

| Optimizasyon | AÃ§Ä±klama |
|-------------|----------|
| **4-bit Quantization** | TÃ¼m modeller 4-bit ile yÃ¼klenir |
| **Gradient Checkpointing** | `mx.checkpoint(model, checkpoints=8)` ile VRAM tasarrufu |
| **Dynamic Batch Sizing** | PPO: 8 (Critic VRAM), GRPO: 16 (available memory'ye gÃ¶re) |
| **Memory Profiler** | Her 50 step'te `mx.metal.get_active_memory()` ile real-time monitoring |
| **LoRA** | Rank=16, sadece belirli layer'lar eÄŸitilir |

---

## ğŸš€ **BaÅŸlangÄ±Ã§ KomutlarÄ±**

### Faz 1: SFT (Ortak)

```bash
# Custom training loop implementation
python src/train_sft.py \
    --model google/gemma-2b-it \
    --data data/train.jsonl \
    --iters 5000 --batch-size 4 --lora-layers 16 \
    --rank 16 --learning-rate 2e-4 --quantize 4bit \
    --adapter-path checkpoints/sft_adapter

# Custom fusion script
python src/fuse_model.py \
    --model google/gemma-2b-it \
    --adapter-path checkpoints/sft_adapter \
    --save-path checkpoints/sft_merged_model
```

### Faz 2: RM Training (Bradley-Terry)

```bash
python train_rm_bt.py \
    --model ./sft_merged_model \
    --data HuggingFaceH4/ultrafeedback_binarized \
    --batch-size 8 --epochs 2 --learning-rate 1e-4 \
    --lora-rank 8 --quantize 4bit --output ./rm_model_bt
```

### Faz 3: PPO / GRPO Training

```bash
# PPO (seed=42)
python train_ppo.py \
    --actor-model ./sft_merged_model --rm-model ./rm_model_bt \
    --seed 42 --num-iterations 800 --clip-range 0.2 \
    --value-coef 0.5 --entropy-coef 0.01 --kl-schedule phased \
    --output ./ppo_model_seed42

# GRPO (seed=42)
python train_grpo.py \
    --actor-model ./sft_merged_model --rm-model ./rm_model_bt \
    --seed 42 --K 6 --num-iterations 800 --clip-range 0.2 \
    --kl-schedule phased --output ./grpo_model_seed42

# DiÄŸer seed'ler iÃ§in --seed ve --output deÄŸiÅŸtir: [42, 123, 777]
```

### KarÅŸÄ±laÅŸtÄ±rma Analizi

```bash
python compare_results.py \
    --ppo-logs  ./logs/ppo_seed{42,123,777}.jsonl \
    --grpo-logs ./logs/grpo_seed{42,123,777}.jsonl \
    --metrics win_rate_vs_sft kl_divergence perplexity training_time vram_peak \
    --output ./comparison_report.pdf
```

---

## ğŸ“ˆ **Beklenen SonuÃ§lar**

| Faz | Metrik | Beklenti |
|-----|--------|----------|
| **1. SFT** | Perplexity | 3.5 â†’ 2.8 (~4 saat) |
| **2. RM** | Pairwise Accuracy | â‰¥ 0.70 (~2 saat) |
| **3. PPO** | Win Rate, KL, VRAM | TBD (~18â€“24 saat, ~4â€“5 GB VRAM) |
| **4. GRPO** | Win Rate, KL, VRAM | TBD (~12â€“15 saat, ~2.5 GB VRAM) |

> **Not:** Nihai karÅŸÄ±laÅŸtÄ±rma sonuÃ§larÄ±, comparison protocol tamamlandÄ±ktan sonra eklenecektir.

---

## âš ï¸ **Bilinen Limitasyonlar**

**Her iki yÃ¶ntem iÃ§in:**
- **Offline Data:** UltraFeedback statik, iterative data improvement sÄ±nÄ±rlÄ±.
- **RM BaÄŸÄ±mlÄ±lÄ±ÄŸÄ±:** Bradley-Terry RM kalitesi her iki yÃ¶ntemin baÅŸarÄ±sÄ±nÄ± belirler.
- **Score Drift:** Running normalization gerekli.

**PPO'ya Ã¶zgÃ¼:**
- Critic overfitting riski (kÃ¼Ã§Ã¼k batch)
- Ä°ki model koordinasyonu (Actor + Critic gÃ¼ncelleme sÄ±rasÄ±)

**GRPO'ya Ã¶zgÃ¼:**
- Group size trade-off (K bÃ¼yÃ¼k â†’ yavaÅŸ, K kÃ¼Ã§Ã¼k â†’ yÃ¼ksek variance)
- Run-to-run tutarsÄ±zlÄ±k (stochastic group sampling)

---

## ğŸ§© **Pipeline Summary**

| Faz | Girdi | Teknik | Ã‡Ä±ktÄ± |
|-----|-------|--------|-------|
| **1. SFT** | UltraFeedback (Chosen) | QLoRA (4-bit, rank=16) | `sft_merged_model` |
| **2. RM** | UltraFeedback (Chosen/Rejected) | Bradley-Terry Loss | `rm_model_bt` |
| **3. PPO** | Prompts + RM | Actor-Critic + GAE + PPO Clip | `ppo_model_best` |
| **4. GRPO** | Prompts + RM | Group Sampling (K=6) + Clipped Surrogate | `grpo_model_best` |

### ğŸ¯ Model Lineage

```
Base Gemma 2B (4-bit)
    â†“
[FAZ 1: SFT]
    â†“
SFT-Gemma â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“               â†“                                  â†“
[FAZ 2: RM]    [FAZ 3: PPO]                     [FAZ 4: GRPO]
(BT Loss)      Actor + Critic (training)         Actor (training)
    â†“           Reference (frozen)                Reference (frozen)
  rm_model_bt   BT RM (frozen)                    BT RM (frozen)
                      â†“                                  â†“
                PPO Model (best)               GRPO Model (best)
                      â†“                                  â†“
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â†“
                            ğŸ“Š Comparison Analysis
                          (Cohen's d + GPT-4o-mini)
```

---

## ğŸ“ **Kaynaklar**

1. **PPO:** Schulman et al. "Proximal Policy Optimization Algorithms" (2017)
2. **GAE:** Schulman et al. "High-Dimensional Continuous Control Using Generalized Advantage Estimation" (2016)
3. **GRPO:** DeepSeek-Math Paper (2024)
4. **Bradley-Terry:** Bradley & Terry, "Rank Analysis of Incomplete Block Designs" (1952)
5. **UltraFeedback:** Cui et al. "UltraFeedback" (2023)
6. **Cohen's d:** Cohen, "Statistical Power Analysis for the Behavioral Sciences" (1988)
7. **MLX Framework:** Apple MLX Documentation
8. **GPT-4o-mini:** OpenAI (2024)

---