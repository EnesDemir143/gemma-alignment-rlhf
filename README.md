# ğŸ”¬ Gemma Alignment: PPO vs GRPO Comparison Study

A controlled experiment comparing two RLHF alignment methods â€” **PPO** (Proximal Policy Optimization) and **GRPO** (Group Relative Policy Optimization) â€” through a full fine-tuning pipeline on `Gemma 2B-IT` using UltraFeedback preference data.

## ğŸ¯ What This Project Does

This project implements a **complete RLHF pipeline from scratch** and runs both alignment algorithms under identical conditions to produce a fair, reproducible comparison.

## ğŸ”— Pipeline Overview

```text
Gemma 2B (4-bit)
    â”‚
    â–¼
 SFT (QLoRA) â”€â”€â–º sft_merged_model
    â”‚
    â”œâ”€â”€â–º Bradley-Terry RM â”€â”€â–º rm_model_bt
    â”‚
    â”œâ”€â”€â–º PPO  (Actor + Critic + GAE)
    â”‚
    â””â”€â”€â–º GRPO (Group Sampling, K=6)
                â”‚
                â–¼
        ğŸ“Š Comparison (Cohen's d + GPT-4o-mini)
```

### Pipeline Phases

| Phase | Technique | Purpose |
|-------|-----------|---------|
| **1. SFT** | QLoRA (4-bit, rank 16) | Warm-up fine-tuning on chosen responses |
| **2. Reward Model** | Bradley-Terry (pairwise preference) | Learn scalar reward from chosen/rejected pairs |
| **3A. PPO** | Actor-Critic + GAE + Clipped Surrogate | Alignment with learned value function |
| **3B. GRPO** | Group Sampling (K=6) + Clipped Surrogate | Critic-free alignment via group-relative advantages |

> [!NOTE]
> All phases are implemented via custom training loops (not CLI tools) to ensure full control over the PPO/GRPO logic and observability.

## ğŸ“¦ Dataset

- [HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) â€” A processed version of UltraFeedback containing binarized preference pairs (chosen/rejected).

### Data Transformation (`download_data.py`)

The `download_data.py` script downloads the `HuggingFaceH4/ultrafeedback_binarized` dataset, samples a subset, and transforms it into the GenRM-style JSONL format for SFT training:

```
HuggingFaceH4/ultrafeedback_binarized      train.jsonl
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ prompt               â”‚â”€â”€â”                â”‚ messages[0] (role: "user")       â”‚
â”‚ chosen[-1].content   â”‚â”€â”€â”¤â”€â”€ concat â”€â”€â–º   â”‚   "User: {prompt}\n\n            â”‚
â”‚                      â”‚  â”‚                â”‚    Assistant: {chosen}\n\n       â”‚
â”‚                      â”‚  â”‚                â”‚    Analyze the quality..."       â”‚
â”‚ score_chosen         â”‚â”€â”€â”˜â”€â”€ format â”€â”€â–º   â”‚ messages[1] (role: "assistant")  â”‚
â”‚                      â”‚                   â”‚   "Score: {score}/10. ..."       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| `train.jsonl` Field | Source | Content |
|---------------------|--------|---------|
| `messages[0]` (user) | `prompt` + `chosen` response | Original prompt + chosen response + "Analyze the quality..." instruction |
| `messages[1]` (assistant) | `score_chosen` | `"Score: {score:.1f}/10. The response is helpful, harmless, and honest."` |
| Score | `score_chosen` only | Parsed via regex `Score:\s*([0-9]+(?:\.[0-9]+)?)/10` |

> [!NOTE]
> Only `score_chosen` is used â€” the rejected response score is not included in the training data. The "user" message contains both the prompt **and** the chosen response concatenated together.

## ğŸ”‘ Key Techniques

### Bradley-Terry Reward Model

Pairwise preference modeling trained on UltraFeedback `chosen` / `rejected` pairs. Learns a scalar reward function `r(prompt, response)` that scores any generation, providing the training signal for both PPO and GRPO.

> [!NOTE]
> **Supervised Training:** This phase trains the "Judge" (Score Model) using labeled data, preparing it to guide the "Player" (Actor Model) in the subsequent RL phases.

### PPO (Proximal Policy Optimization)

- **Actor-Critic** architecture with a separate value head for advantage estimation
- **GAE** (Generalized Advantage Estimation, Î»=0.95) for low-variance advantage computation
- **EOS-only reward shaping** to bridge scalar RM output with per-token credit assignment
- **Adaptive KL controller** with phased schedule (0.05 â†’ 0.02 â†’ 0.01) to prevent policy drift

> [!TIP]
> **Why PPO?** It uses a "Critic" model to predict scores and "Clipping" to prevent dangerous policy updates, ensuring safe and stable learning.

### GRPO (Group Relative Policy Optimization)

- **Critic-free** â€” no value network needed, reducing memory overhead
- Generates **K=6** responses per prompt, computes group-relative advantages: `Aáµ¢ = ráµ¢ âˆ’ mean(r)`

> [!TIP]
> **Why GRPO?** It removes the "Critic" model entirely, saving massive VRAM. Instead of predicting scores, it generates multiple responses and uses the *group average* as the baseline.
- **EMA-based score normalization** to prevent reward drift during training
- Same adaptive KL controller and clipped surrogate objective as PPO

## ğŸ“Š Comparison Design

Both methods are evaluated under **identical controlled conditions**:

- Same SFT base model and frozen Bradley-Terry RM
- Same training data, hyperparameters, and iteration count
- **3 random seeds** `[42, 123, 777]` per method â†’ 6 total experiment runs
- **Cohen's d** as primary effect size metric (robust with small N)

> [!IMPORTANT]
> **Effect Size vs Significance:** With only 3 seeds, statistical significance (p-value) is hard to prove. **Cohen's d** measures the *magnitude* of the difference (Effect Size), telling us if the win is "meaningful" (d>0.8) or "noisy gain" (d<0.2).

### Evaluation Protocol

| Stage | Metrics |
|-------|---------|
| **During Training** (every 50 iter) | Perplexity, KL Divergence, **Response Length & Reward Score Analysis** |
| **Final Evaluation** | GPT-4o-mini win/loss/tie rate (position-swap debiased) + **95% Confidence Interval** |
| **Statistical Comparison** | Cohen's d across seeds |

> Position-swap debiasing: each comparison is judged in both A-B and B-A order; inconsistencies are counted as ties to eliminate position bias.

## ğŸ“ˆ Expected Trade-offs

| Aspect | PPO | GRPO |
|--------|-----|------|
| **Stability** | âœ… High (GAE + Critic) | âš ï¸ Medium (group variance) |
| **Memory** | âš ï¸ ~4â€“5 GB (Critic overhead) | âœ… ~2.5 GB |
| **Complexity** | âš ï¸ High (Critic + GAE) | âœ… Medium (group sampling) |
| **HP Sensitivity** | âš ï¸ High (clip, value_coef, Î») | âœ… Medium (K, kl_coef) |
| **Final Quality** | TBD | TBD |

## ğŸ—‚ï¸ Project Structure

```
â”œâ”€â”€ src/                  # Training scripts & core modules
â”œâ”€â”€ notebooks/            # EDA & analysis notebooks
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ pipeline.md       # Full technical specification
â”œâ”€â”€ data/                 # Dataset cache
â”œâ”€â”€ artifacts/            # Checkpoints & experiment logs
â””â”€â”€ pyproject.toml
```

## ï¿½ References

1. Schulman et al. â€” *Proximal Policy Optimization Algorithms* (2017)
2. Schulman et al. â€” *High-Dimensional Continuous Control Using GAE* (2016)
3. DeepSeek-Math â€” *GRPO* (2024)
4. Bradley & Terry â€” *Rank Analysis of Incomplete Block Designs* (1952)
5. Cui et al. â€” *UltraFeedback* (2023)
